{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq模型——机器翻译\n",
    "\n",
    "### 案例简介\n",
    "\n",
    "seq2seq是神经机器翻译的主流框架，如今的商用机器翻译系统大多都基于其构建，在本案例中，我们将使用由NIST提供的中英文本数据训练一个简单的中英翻译系统，在实践中学习seq2seq的具体细节，以及了解机器翻译的基本技术。\n",
    "\n",
    "### seq2seq模型\n",
    "\n",
    "从根本上讲，机器翻译需要将输入序列（源语言中的单词）映射到输出序列（目标语言中的单词）。正如我们在课堂上讨论的那样，递归神经网络（RNN）可有效处理此类顺序数据。机器翻译中的一个重要难题是输入和输出序列之间没有一对一的对应关系。即，序列通常具有不同的长度，并且单词对应可以是不平凡的（例如，彼此直接翻译的单词可能不会以相同的顺序出现）。\n",
    "\n",
    "为了解决这个问题，我们将使用一种更灵活的架构，称为seq2seq模型。该模型由编码器和解码器两部分组成，它们都是RNN。编码器将源语言中的单词序列作为输入，并输出RNN层的最终隐藏状态。解码器与之类似，除了它还具有一个附加的全连接层（带有softmax激活），用于定义翻译中下一个单词的概率分布。以此方式，解码器本质上用作目标语言的神经语言模型。关键区别在于，解码器将编码器的输出用作其初始隐藏状态，而不是零向量。\n",
    "\n",
    "### 数据和代码\n",
    "\n",
    "本案例使用了一个小规模的中英平行语料数据，并提供了一个简单的seq2seq模型实现，包括数据的预处理、模型的训练、以及简单的评测。\n",
    "\n",
    "### 评分要求\n",
    "\n",
    "分数由两部分组成，各占50%。第一部分得分为对于简单seq2seq模型的改进，并撰写实验报告，改进方式多样，下一小节会给出一些可能的改进方向。第二分部得分为测试数据的评测结果，我们将给出一个中文测试数据集（`test.txt`），其中每一行为一句中文文本，需要同学提交模型做出的对应翻译结果，助教将对于大家的提交结果统一机器评测，并给出分数。\n",
    "\n",
    "### 改进方向\n",
    "\n",
    "初级改进：\n",
    "- 将RNN模型替换成GRU或者LSTM\n",
    "- 使用双向的encoder获得更好的源语言表示\n",
    "- 对于现有超参数进行调优，这里建议划分出一个开发集，在开发集上进行grid search，并且在报告中汇报开发集结果\n",
    "- 引入更多的训练语料（如果尝试复杂模型，更多的训练数据将非常关键）\n",
    "\n",
    "进阶改进：\n",
    "- 使用注意力机制（注意力机制是一个很重要的NMT技术，建议大家优先进行这方面的尝试，具体有许多种变体，可以参考这个[综述](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)）\n",
    "- 在Encoder部分，使用了字级别的中文输入，可以考虑加入分词的结果，并且将Encoder的词向量替换为预训练过的词向量，获得更好的性能\n",
    "\n",
    "复杂改进：\n",
    "- 使用beam search的技术来帮助更好的解码，对于beam-width进行调优\n",
    "- 将RNN替换为Transformer模型，以及最新的改进变体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import hanlp\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import zhconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用GPU，则将下面的变量设为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = True\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tok/fine\": [\n",
      "    \"商品\",\n",
      "    \"和\",\n",
      "    \"服务\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "result = HanLP('商品和服务', tasks = 'tok')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'translate_train.txt'\n",
    "with open(filename, 'r', encoding = 'utf8') as f:\n",
    "    string = f.read()\n",
    "\n",
    "#print(string)\n",
    "data = string.split('\\n')\n",
    "print(data[0])\n",
    "#data.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "\n",
    "我们将读取目录下的`cn-eng.txt`文件，其中每一行是一个平行句对，例子如下：\n",
    "\n",
    "```\n",
    "我很冷。    I am cold.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于单词进行编号\n",
    "\n",
    "这里引入了两个特殊符号，“SOS”即“Start of sentence”和“EOS”即“End of sentence”。他们会加到输入文本的两端，以控制解码过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "5000\n",
      "10000\n",
      "['继', '续', '看']\n",
      "5063\n"
     ]
    }
   ],
   "source": [
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "SOS_INDEX = 2\n",
    "EOS_INDEX = 3\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.!?，。？]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_sentence_file(filename):\n",
    "    src_sentences_list = []\n",
    "    trg_sentences_list = []\n",
    "    src_vocab = []\n",
    "    trg_vocab = []\n",
    "    num = 0\n",
    "    with open(filename[0], \"r\", encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            num += 1\n",
    "            if num%5000 == 0:\n",
    "                print(num)\n",
    "            src, trg = [normalize_string(s) for s in line.split('\\t')]\n",
    "            #src = zhconv.convert(src, 'zh-hans')\n",
    "            #print(trg)\n",
    "            temp = []\n",
    "#             temp = HanLP(src, tasks = 'tok')['tok/fine']\n",
    "            for w in src:\n",
    "                if w != ' ':\n",
    "                    temp.append(w)\n",
    "                if w not in src_vocab:\n",
    "                    src_vocab.append(w)\n",
    "                    \n",
    "            for w in temp:\n",
    "                if w != ' ' and w not in src_vocab:\n",
    "                    src_vocab.append(w)\n",
    "            src_sentences_list.append(temp)\n",
    "            \n",
    "            temp = []\n",
    "            for w in trg.split(' '):\n",
    "                temp.append(w)\n",
    "                if w not in trg_vocab:\n",
    "                    trg_vocab.append(w)\n",
    "            trg_sentences_list.append(temp)\n",
    "            \n",
    "        new_src_sentences_list, new_trg_sentences_list = [], []\n",
    "        num = 0\n",
    "        for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):\n",
    "            num += 1\n",
    "            if num%5000 == 0:\n",
    "                print(num)\n",
    "            if len(src_sent) <= MAX_LENGTH and len(trg_sent) <= MAX_LENGTH:\n",
    "                new_src_sentences_list.append(src_sent)\n",
    "                new_trg_sentences_list.append(trg_sent)\n",
    "        \n",
    "    src_sentences_list = []\n",
    "    trg_sentences_list = []\n",
    "    with open(filename[1], \"r\", encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            trg, src = [normalize_string(s) for s in line.split('\\t')]\n",
    "            #print(trg)\n",
    "            #print(src)\n",
    "            src = zhconv.convert(src, 'zh-hans')\n",
    "            #print(trg)\n",
    "            temp = []\n",
    "#             temp = HanLP(src, tasks = 'tok')['tok/fine']\n",
    "            for w in src:\n",
    "                if w != ' ':\n",
    "                    temp.append(w)\n",
    "                if w not in src_vocab:\n",
    "                    src_vocab.append(w)\n",
    "                    \n",
    "            for w in temp:\n",
    "                if w != ' ' and w not in src_vocab:\n",
    "                    src_vocab.append(w)\n",
    "            src_sentences_list.append(temp)\n",
    "            \n",
    "            temp = []\n",
    "            for w in trg.split(' '):\n",
    "                temp.append(w)\n",
    "                if w not in trg_vocab:\n",
    "                    trg_vocab.append(w)\n",
    "            trg_sentences_list.append(temp)\n",
    "        \n",
    "        num = 0\n",
    "        for src_sent, trg_sent in zip(src_sentences_list, trg_sentences_list):\n",
    "            num += 1\n",
    "            if num%5000 == 0:\n",
    "                print(num)\n",
    "            if len(src_sent) <= MAX_LENGTH and len(trg_sent) <= MAX_LENGTH:\n",
    "                new_src_sentences_list.append(src_sent)\n",
    "                new_trg_sentences_list.append(trg_sent)\n",
    "    \n",
    "    src_vocab.append('<unk>')\n",
    "    trg_vocab.append('<unk>')\n",
    "    return new_src_sentences_list, new_trg_sentences_list, src_vocab, trg_vocab\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n",
    "src_sentences_list, trg_sentences_list, src_vocab, trg_vocab = read_sentence_file(('cn-eng.txt', 'translate_train.txt'))\n",
    "#print(len(src_sentences_list))\n",
    "#print(len(trg_sentences_list))\n",
    "# src_extra_list = []\n",
    "# trg_extra_list = []\n",
    "# for dict_pair in data:\n",
    "#     dict_pair = eval(dict_pair)\n",
    "#     src = normalize_string(dict_pair['chinese'])\n",
    "#     src_list = []\n",
    "#     for w in src:\n",
    "#         if w != ' ':\n",
    "#             src_list.append(w)\n",
    "#     if len(src_list) > MAX_LENGTH:\n",
    "#     trg = normalize_string(dict_pair['english'])\n",
    "#     #src_extra_list.append([w for w in src])\n",
    "#     trg_extra_list.append([w for w in trg.split(' ')])\n",
    "\n",
    "# src_sentences_list = src_sentences_list + src_extra_list\n",
    "# trg_sentences_list = trg_sentences_list + trg_extra_list\n",
    "\n",
    "train_src_sentences_list = src_sentences_list[:len(src_sentences_list)//100*95]\n",
    "train_trg_sentences_list = trg_sentences_list[:len(trg_sentences_list)//100*95]\n",
    "val_src_sentences_list = src_sentences_list[len(src_sentences_list)//100*95:]\n",
    "val_trg_sentences_list = trg_sentences_list[len(trg_sentences_list)//100*95:]\n",
    "# for src_sent, trg_sent in zip(val_src_sentences_list, val_trg_sentences_list):\n",
    "#     if src_sent in train_src_sentences_list or trg_sent in train_trg_sentences_list:\n",
    "#         val_src_sentences_list.remove(src_sent)\n",
    "#         val_trg_sentences_list.remove(trg_sent)\n",
    "\n",
    "length = [len(sent) for sent in train_src_sentences_list]\n",
    "\n",
    "print(train_src_sentences_list[-1])\n",
    "print(len(val_src_sentences_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "\n",
    "丢弃除了中文、字母和常用标点之外的符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取平行语料，并进行清理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤句子\n",
    "\n",
    "样例为了加快训练，只保留了不长于10个单词的句对，真正实验中将更多数据考虑进来可能获得更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据的全过程：\n",
    "\n",
    "- 读取数据，每一行分别处理，将其转换成句对\n",
    "- 对于文本进行处理，过滤无用符号\n",
    "- 根据已有文本对于单词进行编号，构建符号到编号的映射\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import numpy as np\n",
    "assert device == \"cuda\"   # use gpu whenever you can!\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "class MTDataset(data.Dataset):\n",
    "    def __init__(self, src_sentences, src_vocab, trg_sentences, trg_vocab, sampling=1.):\n",
    "        self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
    "        self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
    "        \n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        \n",
    "        self.max_src_seq_length = MAX_LENGTH+2   # max length plus eos and sos\n",
    "        self.max_trg_seq_length = MAX_LENGTH+2\n",
    "\n",
    "        self.src_v2id = {v : i for i, v in enumerate(src_vocab)}\n",
    "        self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
    "        self.trg_v2id = {v : i for i, v in enumerate(trg_vocab)}\n",
    "        self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_sent = self.src_sentences[index]\n",
    "        src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n",
    "        src_id = []\n",
    "        for w in src_sent:\n",
    "            if w not in self.src_vocab:\n",
    "                w = '<UNK>'\n",
    "            src_id.append(self.src_v2id[w])\n",
    "        src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
    "                  (self.max_src_seq_length - src_len))\n",
    "\n",
    "        trg_sent = self.trg_sentences[index]\n",
    "        trg_len = len(trg_sent) + 2\n",
    "        trg_id = []\n",
    "        for w in trg_sent:\n",
    "            if w not in self.trg_vocab:\n",
    "                w = '<UNK>'\n",
    "            trg_id.append(self.trg_v2id[w])\n",
    "        trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
    "                  (self.max_trg_seq_length - trg_len))\n",
    "\n",
    "        return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本数据转换为张量\n",
    "\n",
    "为了训练，我们需要将句子变成神经网络可以理解的东西（数字）。每个句子将被分解成单词，然后变成张量，其中每个单词都被索引替换（来自之前的Lang索引）。在创建这些张量时，我们还将附加EOS令牌以表示该句子已结束。\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    if lang.name == 'cn':\n",
    "        #sentence_list = HanLP(sentence, tasks = 'tok')['tok/fine'] \n",
    "        return [lang.word2index[word] for word in sentence]\n",
    "    else:\n",
    "        return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = torch.LongTensor(indexes).view(-1, 1)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组件模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout = 0.):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        pack_pad = pack_padded_sequence(self.dropout(self.embedding(inputs)), lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, finals = self.rnn(pack_pad)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length = inputs.size(1))\n",
    "        return outputs, finals\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # A torch module implementing an LSTM. The `forward` function should just\n",
    "    # perform one step of update and output logits before softmax.\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, bidirection = True, dropout=0.5):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        #print(input_size)\n",
    "        #self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.bidirection = bidirection\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True,\n",
    "                      dropout=dropout, bidirectional=self.bidirection)\n",
    "        \n",
    "        self.bi_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        pack_pad = pack_padded_sequence(self.dropout(self.embedding(inputs)), lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, finals = self.gru(pack_pad)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length = inputs.size(1))\n",
    "        \n",
    "        if self.bidirection:\n",
    "            forward = finals[0:list(finals.size())[0]:2]\n",
    "            backward = finals[1:list(finals.size())[0]:2]\n",
    "            finals = torch.cat([forward, backward], dim=2)\n",
    "            #print('final after')\n",
    "            #print(finals.size())\n",
    "            finals = self.bi_linear(finals)\n",
    "            outputs = self.bi_linear(outputs)\n",
    "        \n",
    "        return outputs, finals\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and memory states.\n",
    "        if self.bidirection == True:\n",
    "            return torch.zeros(self.num_layers*2, 1, self.hidden_size)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        # Note: we run this one step at a time  \n",
    "        #hidden = hidden[-1].unsqueeze(1)\n",
    "        embed_input = self.dropout(self.embedding(inputs))\n",
    "        #print(embed_input.size())\n",
    "        #print(hidden.size())\n",
    "        rnn_output, hidden = self.rnn(embed_input, hidden)\n",
    "\n",
    "        #rnn_output = rnn_output.squeeze(0)\n",
    "        #output = F.log_softmax(self.out(rnn_output))\n",
    "\n",
    "        return rnn_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.q_weight = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.k_weight = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        #self.va = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.att_weight = None\n",
    "        \n",
    "    def forward(self, q, k, mask):\n",
    "        \"\"\"\n",
    "        q: decoder hidden\n",
    "        k: encoder outputs\n",
    "        v: encoder outputs\n",
    "        \"\"\"\n",
    "        #q = q.unsqueeze(1)\n",
    "        #q_linear = nn.Linear(q.size(2), self.hidden_size).cuda()\n",
    "        #k_linear = nn.Linear(k.size(2), self.hidden_size).cuda()\n",
    "        #print(list(q_p.size()))\n",
    "        #k_p = self.k_weight(k)\n",
    "        #print(list(k.size()))\n",
    "        combine = torch.tanh(self.q_weight(q) + self.k_weight(k))\n",
    "        energy = torch.matmul(self.v, torch.transpose(combine, 1, 2))\n",
    "        #energy = torch.bmm(combine, torch.transpose(v, 1, 2))\n",
    "        energy.data.masked_fill_(mask == False, -float('inf'))\n",
    "        att_weight = F.softmax(energy, dim=-1)\n",
    "        self.att_weight = att_weight\n",
    "        context = torch.bmm(att_weight, k)\n",
    "        return context\n",
    "\n",
    "'''\n",
    "AttentionDecoder\n",
    "'''\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"An attention-based RNN decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, output_size, hidden_size, num_layers, feed_ratio = 0.5, dropout=0., attention = None):\n",
    "        \"\"\"\n",
    "          Inputs:\n",
    "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
    "        - `attention`: this is your self-defined Attention object. You can\n",
    "            either define an individual class for your Attention and pass it\n",
    "            here or leave `attention` as None and just implement everything\n",
    "            here.\n",
    "        \"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        ### Your code here!\n",
    "        self.hidden_size = hidden_size\n",
    "        #self.input_size = input_size\n",
    "        self.dropout_p = dropout\n",
    "        self.feed_ratio = feed_ratio\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = attention\n",
    "        \n",
    "        #self.input_hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm_attn = nn.LSTM(hidden_size*2, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.output1 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.output2 = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, hidden, encoder_outputs, encoder_final, src_mask):\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        if self.attention != None:\n",
    "            context = self.attention(hidden[-1].unsqueeze(1), encoder_outputs, src_mask)\n",
    "            embed_input = self.embedding(inputs)\n",
    "            #print(embed_input.size())\n",
    "            lstm_input = torch.cat([context, self.dropout(embed_input)], dim = -1)\n",
    "            #print('hidden size')\n",
    "            #print(hidden[0].size())\n",
    "            lstm_output, (hidden, cell) = self.lstm_attn(lstm_input, (hidden, hidden))\n",
    "        else:\n",
    "            embed_input = self.embedding(inputs)\n",
    "            lstm_output, (hidden, cell) = self.lstm(embed_input, (hidden, hidden))\n",
    "        #output = self.out(lstm_output)\n",
    "        \n",
    "\n",
    "        #output_prob = F.log_softmax(output, dim = -1)\n",
    "\n",
    "        #return output_prob, lstm_output, hidden\n",
    "        return lstm_output, hidden\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
    "        state.\"\"\"\n",
    "        #decoder_init_hiddens = None\n",
    "        ### Your code here!\n",
    "\n",
    "        if encoder_final != None:\n",
    "            return torch.tanh(encoder_final)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"A Encoder-Decoder architecture with attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, feed_ratio, generator):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          - `encoder`: an `Encoder` object.\n",
    "          - `decoder`: an `AttentionDecoder` object.\n",
    "          - `src_embed`: an nn.Embedding object representing the lookup table for\n",
    "              input (source) sentences.\n",
    "          - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
    "              output (target) sentences.\n",
    "          - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
    "              the next code cell.\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.feed_ratio = feed_ratio\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src_ids, trg_ids, src_lengths, max_len = MAX_LENGTH+2):\n",
    "        \"\"\"Take in and process masked source and tar get sequences.\n",
    "\n",
    "        Inputs:\n",
    "          `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
    "            a batch of source sentences of word ids.\n",
    "          `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
    "            a batch of target sentences of word ids.\n",
    "          `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
    "            sequence length of `src_ids`.\n",
    "\n",
    "        Returns the decoder outputs, see the above cell.\n",
    "        \"\"\"\n",
    "        ### Your code here!\n",
    "        # You can refer to `EncoderDecoder` and extend from it.\n",
    "        src_mask = (src_ids != 0).unsqueeze(1)\n",
    "        encoder_outputs, encoder_final = self.encoder(src_ids, src_lengths)\n",
    "        \n",
    "        batch_size = src_ids.size()[0]\n",
    "        decoder_input = torch.ones(batch_size, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
    "        #print(decoder_input.size())\n",
    "        decoder_hidden = encoder_final\n",
    "#         if USE_CUDA:\n",
    "#             decoder_input = decoder_input.cuda()\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(max_len-1):\n",
    "            #print(hidden[-1].size())\n",
    "            #print(hidden[-1].unsqueeze(1).size())\n",
    "            #output_prob, decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs, encoder_final, src_mask)\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            feed_target = True if random.random() < self.feed_ratio else False\n",
    "            if feed_target:\n",
    "                decoder_input = trg_ids[:,i+1].unsqueeze(1)\n",
    "            else:\n",
    "                #print(decoder_output[:, -1].size())\n",
    "                #print(decoder_output[:, -1].size())\n",
    "                prob = self.generator(decoder_output[:, -1]).unsqueeze(1)\n",
    "                _, next_word = torch.max(prob, dim = 2)\n",
    "                decoder_input = next_word\n",
    "                #print('topi')\n",
    "                #print(topi.size())\n",
    "                #decoder_input = topi.squeeze(2)\n",
    "            \n",
    "            outputs.append(decoder_output)\n",
    "    \n",
    "        #print(len(outputs))\n",
    "        #print(outputs[0].size())\n",
    "        outputs = torch.cat(outputs, dim = 1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAttentionDecoder(nn.Module):\n",
    "    \"\"\"A Encoder-Decoder architecture with attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, feed_ratio, generator):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          - `encoder`: an `Encoder` object.\n",
    "          - `decoder`: an `AttentionDecoder` object.\n",
    "          - `src_embed`: an nn.Embedding object representing the lookup table for\n",
    "              input (source) sentences.\n",
    "          - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
    "              output (target) sentences.\n",
    "          - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
    "              the next code cell.\n",
    "        \"\"\"\n",
    "        super(EncoderAttentionDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.feed_ratio = feed_ratio\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src_ids, trg_ids, src_lengths, max_len = MAX_LENGTH+2):\n",
    "        \"\"\"Take in and process masked source and tar get sequences.\n",
    "\n",
    "        Inputs:\n",
    "          `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
    "            a batch of source sentences of word ids.\n",
    "          `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
    "            a batch of target sentences of word ids.\n",
    "          `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
    "            sequence length of `src_ids`.\n",
    "\n",
    "        Returns the decoder outputs, see the above cell.\n",
    "        \"\"\"\n",
    "        ### Your code here!\n",
    "        # You can refer to `EncoderDecoder` and extend from it.\n",
    "        src_mask = (src_ids != 0).unsqueeze(1)\n",
    "        encoder_outputs, encoder_final = self.encoder(src_ids, src_lengths)\n",
    "        \n",
    "        batch_size = src_ids.size()[0]\n",
    "        decoder_input = torch.ones(batch_size, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
    "        #print(decoder_input.size())\n",
    "        decoder_hidden = encoder_final\n",
    "#         if USE_CUDA:\n",
    "#             decoder_input = decoder_input.cuda()\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(max_len-1):\n",
    "            #print(hidden[-1].size())\n",
    "            #print(hidden[-1].unsqueeze(1).size())\n",
    "            #output_prob, decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs, encoder_final, src_mask)\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs, encoder_final, src_mask)\n",
    "            feed_target = True if random.random() < self.feed_ratio else False\n",
    "            if feed_target:\n",
    "                decoder_input = trg_ids[:,i+1].unsqueeze(1)\n",
    "            else:\n",
    "                #print(decoder_output[:, -1].size())\n",
    "                prob = self.generator(decoder_output[:, -1]).unsqueeze(1)\n",
    "                _, next_word = torch.max(prob, dim = 2)\n",
    "                decoder_input = next_word\n",
    "                #print('topi')\n",
    "                #print(topi.size())\n",
    "                #decoder_input = topi.squeeze(2)\n",
    "            \n",
    "            outputs.append(decoder_output)\n",
    "    \n",
    "        #print(len(outputs))\n",
    "        #print(outputs[0].size())\n",
    "        outputs = torch.cat(outputs, dim = 1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
    "hidden_size = 200  # RNN hidden size.\n",
    "dropout = 0.2\n",
    "num_layers = 3\n",
    "bidirection = True\n",
    "feed_ratio = 0.2\n",
    "learning_rate = 2e-4\n",
    "clipping_value = 2 \n",
    "num_epochs = 60\n",
    "MAX_LENGTH = 20\n",
    "early_stop = 5\n",
    "\n",
    "\n",
    "train_set = MTDataset(train_src_sentences_list, src_vocab,\n",
    "                      train_trg_sentences_list, trg_vocab, sampling=1.)\n",
    "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                    num_workers=8, shuffle=True)\n",
    "\n",
    "val_set = MTDataset(val_src_sentences_list, src_vocab,\n",
    "                    val_trg_sentences_list, trg_vocab, sampling=1.)\n",
    "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "## 一次训练迭代\n",
    "\n",
    "为了训练，我们首先通过编码器逐字运行输入语句，并跟踪每个输出和最新的隐藏状态。接下来，为解码器提供解码器的最后一个隐藏状态作为其第一隐藏状态，并向其提供`<SOS>`作为其第一输入。从那里开始，我们迭代地预测来自解码器的下一个单词。\n",
    "    \n",
    "### Teacher Forcing 和 Scheduled Sampling\n",
    "\n",
    "\"Teacher Forcing\"指的是每次都基于完全准确的上文进行解码，这样训练模型收敛很快，但是会造成实际场景和训练场景有较大差别，因为实际场景上文也都是模型预测的，可能不准确，具体细节可参考[论文](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf)。\n",
    "\n",
    "观察Teacher Forcing的网络的输出，我们可以看到该网络语法连贯，但是偏离正确的翻译。可以将其为学会了如何听老师的指示，而未学习如何独自冒险。\n",
    "\n",
    "解决强迫教师问题的方法称为“计划抽样”（[Scheduled Sampling](https://arxiv.org/abs/1506.03099)），它在训练时仅在使用目标值和预测值之间进行切换。我们将在训练时随机选择,有时我们将使用真实目标作为输入（忽略解码器的输出），有时我们将使用解码器的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None, c_value = None, model = None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        self.clipping_value = c_value\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:  # training mode\n",
    "            loss.backward()    \n",
    "            ########################## clip\n",
    "            torch.nn.utils.clip_grad_norm(self.model.parameters(), self.clipping_value)\n",
    "            ##########################\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm\n",
    "\n",
    "\n",
    "def run_epoch(data_loader, model, loss_compute, print_every):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
    "        # We define some notations here to help you understand the loaded tensor\n",
    "        # shapes:\n",
    "        #   `B`: batch size\n",
    "        #   `T`: max sequence length of source sentences\n",
    "        #   `L`: max sequence length of target sentences; due to our preprocessing\n",
    "        #        in the beginning, `L` == `T` == 50\n",
    "        # An example of `src_ids_BxT` (when B = 2):\n",
    "        #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
    "        #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
    "        # The corresponding `src_lengths_B` would be [47, 49].\n",
    "        # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
    "\n",
    "        src_ids_BxT = src_ids_BxT.to(device)\n",
    "        src_lengths_B = src_lengths_B.to(device)\n",
    "        trg_ids_BxL = trg_ids_BxL.to(device)\n",
    "        del trg_lengths_B   # unused\n",
    "\n",
    "        output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
    "        #print(output.size())\n",
    "        #print(trg_ids_BxL[:,1:].size())\n",
    "        loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
    "                            norm=src_ids_BxT.size(0))\n",
    "        total_loss += loss\n",
    "        total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
    "\n",
    "        if model.training and i % print_every == 0:\n",
    "            print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))\n",
    "\n",
    "\n",
    "def train(model, num_epochs, learning_rate, print_every, clipping_value, early_stop):\n",
    "    # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
    "    # computing the loss.\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Keep track of dev ppl for each epoch.\n",
    "    dev_ppls = []\n",
    "    min_ppls = float('inf')\n",
    "    stop = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "\n",
    "        model.train()\n",
    "        train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
    "                              loss_compute=SimpleLossCompute(model.generator, criterion, optim, clipping_value, model), print_every=print_every)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            dev_ppl = run_epoch(data_loader=val_data_loader, model=model, loss_compute=SimpleLossCompute(model.generator, criterion, None, None), print_every=print_every)\n",
    "            print(\"Validation perplexity: %f\" % dev_ppl)\n",
    "            if dev_ppl < min_ppls:\n",
    "                min_ppls = dev_ppl\n",
    "                stop = 0\n",
    "            else:\n",
    "                stop +=1\n",
    "            \n",
    "            dev_ppls.append(dev_ppl)\n",
    "            \n",
    "            if stop > early_stop:\n",
    "                break\n",
    "        \n",
    "    return dev_ppls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是用于辅助输出训练情况的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingye/.conda/envs/opence/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/mingye/.conda/envs/opence/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 0 Loss: 85.156792\n",
      "Epoch Step: 100 Loss: 47.402527\n",
      "Epoch Step: 200 Loss: 46.616879\n",
      "Epoch Step: 300 Loss: 44.978535\n",
      "Epoch Step: 400 Loss: 45.718906\n",
      "Epoch Step: 500 Loss: 44.317425\n",
      "Epoch Step: 600 Loss: 47.137100\n",
      "Epoch Step: 700 Loss: 42.280804\n",
      "Validation perplexity: 134.147590\n",
      "Epoch 1\n",
      "Epoch Step: 0 Loss: 44.436398\n",
      "Epoch Step: 100 Loss: 39.273205\n",
      "Epoch Step: 200 Loss: 41.056782\n",
      "Epoch Step: 300 Loss: 42.482124\n",
      "Epoch Step: 400 Loss: 45.122105\n",
      "Epoch Step: 500 Loss: 41.393913\n",
      "Epoch Step: 600 Loss: 41.293709\n",
      "Epoch Step: 700 Loss: 39.675114\n",
      "Validation perplexity: 94.182579\n",
      "Epoch 2\n",
      "Epoch Step: 0 Loss: 41.052776\n",
      "Epoch Step: 100 Loss: 41.820160\n",
      "Epoch Step: 200 Loss: 41.632366\n",
      "Epoch Step: 300 Loss: 42.706738\n",
      "Epoch Step: 400 Loss: 39.876827\n",
      "Epoch Step: 500 Loss: 40.789352\n",
      "Epoch Step: 600 Loss: 41.988914\n",
      "Epoch Step: 700 Loss: 42.029194\n",
      "Validation perplexity: 73.178903\n",
      "Epoch 3\n",
      "Epoch Step: 0 Loss: 36.936195\n",
      "Epoch Step: 100 Loss: 40.452415\n",
      "Epoch Step: 200 Loss: 41.109303\n",
      "Epoch Step: 300 Loss: 38.154190\n",
      "Epoch Step: 400 Loss: 41.776482\n",
      "Epoch Step: 500 Loss: 38.964741\n",
      "Epoch Step: 600 Loss: 39.391281\n",
      "Epoch Step: 700 Loss: 37.988640\n",
      "Validation perplexity: 61.262680\n",
      "Epoch 4\n",
      "Epoch Step: 0 Loss: 39.068111\n",
      "Epoch Step: 100 Loss: 40.306732\n",
      "Epoch Step: 200 Loss: 37.954124\n",
      "Epoch Step: 300 Loss: 37.897652\n",
      "Epoch Step: 400 Loss: 38.044632\n",
      "Epoch Step: 500 Loss: 38.375252\n",
      "Epoch Step: 600 Loss: 35.575218\n",
      "Epoch Step: 700 Loss: 39.230518\n",
      "Validation perplexity: 52.874429\n",
      "Epoch 5\n",
      "Epoch Step: 0 Loss: 36.950264\n",
      "Epoch Step: 100 Loss: 35.734699\n",
      "Epoch Step: 200 Loss: 36.370052\n",
      "Epoch Step: 300 Loss: 33.464691\n",
      "Epoch Step: 400 Loss: 34.567558\n",
      "Epoch Step: 500 Loss: 35.497662\n",
      "Epoch Step: 600 Loss: 37.782902\n",
      "Epoch Step: 700 Loss: 33.089561\n",
      "Validation perplexity: 46.001297\n",
      "Epoch 6\n",
      "Epoch Step: 0 Loss: 31.271704\n",
      "Epoch Step: 100 Loss: 35.083012\n",
      "Epoch Step: 200 Loss: 35.206001\n",
      "Epoch Step: 300 Loss: 32.148678\n",
      "Epoch Step: 400 Loss: 34.565430\n",
      "Epoch Step: 500 Loss: 34.306316\n",
      "Epoch Step: 600 Loss: 35.081944\n",
      "Epoch Step: 700 Loss: 33.999081\n",
      "Validation perplexity: 39.680316\n",
      "Epoch 7\n",
      "Epoch Step: 0 Loss: 34.342064\n",
      "Epoch Step: 100 Loss: 31.432861\n",
      "Epoch Step: 200 Loss: 33.068027\n",
      "Epoch Step: 300 Loss: 31.657528\n",
      "Epoch Step: 400 Loss: 33.666660\n",
      "Epoch Step: 500 Loss: 33.255196\n",
      "Epoch Step: 600 Loss: 32.185215\n",
      "Epoch Step: 700 Loss: 36.825737\n",
      "Validation perplexity: 35.580600\n",
      "Epoch 8\n",
      "Epoch Step: 0 Loss: 35.229027\n",
      "Epoch Step: 100 Loss: 33.851490\n",
      "Epoch Step: 200 Loss: 35.780506\n",
      "Epoch Step: 300 Loss: 32.602474\n",
      "Epoch Step: 400 Loss: 32.846626\n",
      "Epoch Step: 500 Loss: 32.782547\n",
      "Epoch Step: 600 Loss: 32.739899\n",
      "Epoch Step: 700 Loss: 31.531891\n",
      "Validation perplexity: 30.956909\n",
      "Epoch 9\n",
      "Epoch Step: 0 Loss: 30.389271\n",
      "Epoch Step: 100 Loss: 29.471357\n",
      "Epoch Step: 200 Loss: 32.028759\n",
      "Epoch Step: 300 Loss: 33.624886\n",
      "Epoch Step: 400 Loss: 32.917622\n",
      "Epoch Step: 500 Loss: 30.154438\n",
      "Epoch Step: 600 Loss: 32.361515\n",
      "Epoch Step: 700 Loss: 31.734592\n",
      "Validation perplexity: 27.146693\n",
      "Epoch 10\n",
      "Epoch Step: 0 Loss: 30.962225\n",
      "Epoch Step: 100 Loss: 31.054455\n",
      "Epoch Step: 200 Loss: 31.251450\n",
      "Epoch Step: 300 Loss: 31.750219\n",
      "Epoch Step: 400 Loss: 31.427511\n",
      "Epoch Step: 500 Loss: 32.341465\n",
      "Epoch Step: 600 Loss: 29.688929\n",
      "Epoch Step: 700 Loss: 31.647030\n",
      "Validation perplexity: 24.650583\n",
      "Epoch 11\n",
      "Epoch Step: 0 Loss: 31.604532\n",
      "Epoch Step: 100 Loss: 32.285133\n",
      "Epoch Step: 200 Loss: 28.098455\n",
      "Epoch Step: 300 Loss: 33.926064\n",
      "Epoch Step: 400 Loss: 29.556673\n",
      "Epoch Step: 500 Loss: 28.961868\n",
      "Epoch Step: 600 Loss: 31.659286\n",
      "Epoch Step: 700 Loss: 29.698456\n",
      "Validation perplexity: 22.138752\n",
      "Epoch 12\n",
      "Epoch Step: 0 Loss: 28.585350\n",
      "Epoch Step: 100 Loss: 31.178745\n",
      "Epoch Step: 200 Loss: 29.668255\n",
      "Epoch Step: 300 Loss: 29.665646\n",
      "Epoch Step: 400 Loss: 30.807344\n",
      "Epoch Step: 500 Loss: 30.314760\n",
      "Epoch Step: 600 Loss: 26.562136\n",
      "Epoch Step: 700 Loss: 29.030941\n",
      "Validation perplexity: 19.865386\n",
      "Epoch 13\n",
      "Epoch Step: 0 Loss: 25.969877\n",
      "Epoch Step: 100 Loss: 30.062477\n",
      "Epoch Step: 200 Loss: 30.625204\n",
      "Epoch Step: 300 Loss: 29.144234\n",
      "Epoch Step: 400 Loss: 30.205103\n",
      "Epoch Step: 500 Loss: 30.775290\n",
      "Epoch Step: 600 Loss: 27.687704\n",
      "Epoch Step: 700 Loss: 28.226852\n",
      "Validation perplexity: 18.123106\n",
      "Epoch 14\n",
      "Epoch Step: 0 Loss: 27.826948\n",
      "Epoch Step: 100 Loss: 27.388697\n",
      "Epoch Step: 200 Loss: 27.265318\n",
      "Epoch Step: 300 Loss: 27.553967\n",
      "Epoch Step: 400 Loss: 25.182837\n",
      "Epoch Step: 500 Loss: 29.465832\n",
      "Epoch Step: 600 Loss: 26.916811\n",
      "Epoch Step: 700 Loss: 26.815508\n",
      "Validation perplexity: 16.684576\n",
      "Epoch 15\n",
      "Epoch Step: 0 Loss: 27.656954\n",
      "Epoch Step: 100 Loss: 28.336605\n",
      "Epoch Step: 200 Loss: 26.478773\n",
      "Epoch Step: 300 Loss: 28.716263\n",
      "Epoch Step: 400 Loss: 26.814196\n",
      "Epoch Step: 500 Loss: 25.508347\n",
      "Epoch Step: 600 Loss: 27.609343\n",
      "Epoch Step: 700 Loss: 30.406063\n",
      "Validation perplexity: 15.191749\n",
      "Epoch 16\n",
      "Epoch Step: 0 Loss: 23.608841\n",
      "Epoch Step: 100 Loss: 26.824799\n",
      "Epoch Step: 200 Loss: 28.063906\n",
      "Epoch Step: 300 Loss: 28.394096\n",
      "Epoch Step: 400 Loss: 25.860674\n",
      "Epoch Step: 500 Loss: 23.971813\n",
      "Epoch Step: 600 Loss: 24.749737\n",
      "Epoch Step: 700 Loss: 25.538803\n",
      "Validation perplexity: 13.761177\n",
      "Epoch 17\n",
      "Epoch Step: 0 Loss: 24.014545\n",
      "Epoch Step: 100 Loss: 24.703506\n",
      "Epoch Step: 200 Loss: 27.136427\n",
      "Epoch Step: 300 Loss: 26.805998\n",
      "Epoch Step: 400 Loss: 26.720306\n",
      "Epoch Step: 500 Loss: 26.193554\n",
      "Epoch Step: 600 Loss: 24.893713\n",
      "Epoch Step: 700 Loss: 26.060160\n",
      "Validation perplexity: 12.959262\n",
      "Epoch 18\n",
      "Epoch Step: 0 Loss: 25.200521\n",
      "Epoch Step: 100 Loss: 27.607811\n",
      "Epoch Step: 200 Loss: 24.050325\n",
      "Epoch Step: 300 Loss: 23.586271\n",
      "Epoch Step: 400 Loss: 24.944370\n",
      "Epoch Step: 500 Loss: 26.098152\n",
      "Epoch Step: 600 Loss: 25.290852\n",
      "Epoch Step: 700 Loss: 23.917690\n",
      "Validation perplexity: 12.017288\n",
      "Epoch 19\n",
      "Epoch Step: 0 Loss: 26.895052\n",
      "Epoch Step: 100 Loss: 26.059456\n",
      "Epoch Step: 200 Loss: 23.855640\n",
      "Epoch Step: 300 Loss: 22.102547\n",
      "Epoch Step: 400 Loss: 26.859852\n",
      "Epoch Step: 500 Loss: 21.906092\n",
      "Epoch Step: 600 Loss: 24.181698\n",
      "Epoch Step: 700 Loss: 23.548635\n",
      "Validation perplexity: 11.293917\n",
      "Epoch 20\n",
      "Epoch Step: 0 Loss: 26.318214\n",
      "Epoch Step: 100 Loss: 25.470268\n",
      "Epoch Step: 200 Loss: 22.352259\n",
      "Epoch Step: 300 Loss: 22.611013\n",
      "Epoch Step: 400 Loss: 22.349230\n",
      "Epoch Step: 500 Loss: 24.252665\n",
      "Epoch Step: 600 Loss: 22.421122\n",
      "Epoch Step: 700 Loss: 24.527504\n",
      "Validation perplexity: 10.632024\n",
      "Epoch 21\n",
      "Epoch Step: 0 Loss: 25.300489\n",
      "Epoch Step: 100 Loss: 23.807215\n",
      "Epoch Step: 200 Loss: 25.451437\n",
      "Epoch Step: 300 Loss: 23.888472\n",
      "Epoch Step: 400 Loss: 24.357117\n",
      "Epoch Step: 500 Loss: 25.398569\n",
      "Epoch Step: 600 Loss: 26.515312\n",
      "Epoch Step: 700 Loss: 25.209206\n",
      "Validation perplexity: 9.977376\n",
      "Epoch 22\n",
      "Epoch Step: 0 Loss: 23.382792\n",
      "Epoch Step: 100 Loss: 21.490458\n",
      "Epoch Step: 200 Loss: 23.174400\n",
      "Epoch Step: 300 Loss: 21.805590\n",
      "Epoch Step: 400 Loss: 20.730024\n",
      "Epoch Step: 500 Loss: 26.306070\n",
      "Epoch Step: 600 Loss: 24.597771\n",
      "Epoch Step: 700 Loss: 23.495262\n",
      "Validation perplexity: 9.238186\n",
      "Epoch 23\n",
      "Epoch Step: 0 Loss: 24.226488\n",
      "Epoch Step: 100 Loss: 22.396887\n",
      "Epoch Step: 200 Loss: 23.561892\n",
      "Epoch Step: 300 Loss: 23.263538\n",
      "Epoch Step: 400 Loss: 21.863897\n",
      "Epoch Step: 500 Loss: 25.652792\n",
      "Epoch Step: 600 Loss: 21.240271\n",
      "Epoch Step: 700 Loss: 24.856800\n",
      "Validation perplexity: 8.771007\n",
      "Epoch 24\n",
      "Epoch Step: 0 Loss: 23.798092\n",
      "Epoch Step: 100 Loss: 24.225025\n",
      "Epoch Step: 200 Loss: 24.264402\n",
      "Epoch Step: 300 Loss: 21.323519\n",
      "Epoch Step: 400 Loss: 22.327873\n",
      "Epoch Step: 500 Loss: 23.223780\n",
      "Epoch Step: 600 Loss: 21.157476\n",
      "Epoch Step: 700 Loss: 21.716496\n",
      "Validation perplexity: 8.143167\n",
      "Epoch 25\n",
      "Epoch Step: 0 Loss: 22.290844\n",
      "Epoch Step: 100 Loss: 22.848345\n",
      "Epoch Step: 200 Loss: 19.676819\n",
      "Epoch Step: 300 Loss: 21.326702\n",
      "Epoch Step: 400 Loss: 21.943062\n",
      "Epoch Step: 500 Loss: 22.965181\n",
      "Epoch Step: 600 Loss: 19.911158\n",
      "Epoch Step: 700 Loss: 21.959179\n",
      "Validation perplexity: 7.885240\n",
      "Epoch 26\n",
      "Epoch Step: 0 Loss: 21.682184\n",
      "Epoch Step: 100 Loss: 20.244619\n",
      "Epoch Step: 200 Loss: 21.029215\n",
      "Epoch Step: 300 Loss: 20.358679\n",
      "Epoch Step: 400 Loss: 19.987198\n",
      "Epoch Step: 500 Loss: 21.215504\n",
      "Epoch Step: 600 Loss: 20.488411\n",
      "Epoch Step: 700 Loss: 19.501638\n",
      "Validation perplexity: 7.505603\n",
      "Epoch 27\n",
      "Epoch Step: 0 Loss: 21.580858\n",
      "Epoch Step: 100 Loss: 20.932524\n",
      "Epoch Step: 200 Loss: 22.213947\n",
      "Epoch Step: 300 Loss: 21.399546\n",
      "Epoch Step: 400 Loss: 19.997780\n",
      "Epoch Step: 500 Loss: 20.389818\n",
      "Epoch Step: 600 Loss: 21.141987\n",
      "Epoch Step: 700 Loss: 20.106123\n",
      "Validation perplexity: 7.001964\n",
      "Epoch 28\n",
      "Epoch Step: 0 Loss: 22.011589\n",
      "Epoch Step: 100 Loss: 21.805099\n",
      "Epoch Step: 200 Loss: 23.511053\n",
      "Epoch Step: 300 Loss: 20.726545\n",
      "Epoch Step: 400 Loss: 19.378670\n",
      "Epoch Step: 500 Loss: 19.883770\n",
      "Epoch Step: 600 Loss: 19.601439\n",
      "Epoch Step: 700 Loss: 24.402243\n",
      "Validation perplexity: 6.625152\n",
      "Epoch 29\n",
      "Epoch Step: 0 Loss: 19.328745\n",
      "Epoch Step: 100 Loss: 21.684019\n",
      "Epoch Step: 200 Loss: 20.633175\n",
      "Epoch Step: 300 Loss: 21.501028\n",
      "Epoch Step: 400 Loss: 19.540878\n",
      "Epoch Step: 500 Loss: 19.704945\n",
      "Epoch Step: 600 Loss: 19.831648\n",
      "Epoch Step: 700 Loss: 20.719046\n",
      "Validation perplexity: 6.450127\n",
      "Epoch 30\n",
      "Epoch Step: 0 Loss: 21.671381\n",
      "Epoch Step: 100 Loss: 20.977673\n",
      "Epoch Step: 200 Loss: 22.141483\n",
      "Epoch Step: 300 Loss: 20.029699\n",
      "Epoch Step: 400 Loss: 20.339697\n",
      "Epoch Step: 500 Loss: 22.365788\n",
      "Epoch Step: 600 Loss: 19.660423\n",
      "Epoch Step: 700 Loss: 19.029638\n",
      "Validation perplexity: 6.159144\n",
      "Epoch 31\n",
      "Epoch Step: 0 Loss: 17.015919\n",
      "Epoch Step: 100 Loss: 19.502176\n",
      "Epoch Step: 200 Loss: 18.522449\n",
      "Epoch Step: 300 Loss: 21.862635\n",
      "Epoch Step: 400 Loss: 21.297199\n",
      "Epoch Step: 500 Loss: 19.211176\n",
      "Epoch Step: 600 Loss: 18.797308\n",
      "Epoch Step: 700 Loss: 18.482475\n",
      "Validation perplexity: 5.925662\n",
      "Epoch 32\n",
      "Epoch Step: 0 Loss: 19.064106\n",
      "Epoch Step: 100 Loss: 19.582895\n",
      "Epoch Step: 200 Loss: 18.410906\n",
      "Epoch Step: 300 Loss: 19.588636\n",
      "Epoch Step: 400 Loss: 20.217314\n",
      "Epoch Step: 500 Loss: 19.965338\n",
      "Epoch Step: 600 Loss: 22.132843\n",
      "Epoch Step: 700 Loss: 20.657490\n",
      "Validation perplexity: 5.704011\n",
      "Epoch 33\n",
      "Epoch Step: 0 Loss: 17.947845\n",
      "Epoch Step: 100 Loss: 18.379190\n",
      "Epoch Step: 200 Loss: 18.549026\n",
      "Epoch Step: 300 Loss: 18.775036\n",
      "Epoch Step: 400 Loss: 18.154537\n",
      "Epoch Step: 500 Loss: 16.890245\n",
      "Epoch Step: 600 Loss: 16.976376\n",
      "Epoch Step: 700 Loss: 21.469231\n",
      "Validation perplexity: 5.532382\n",
      "Epoch 34\n",
      "Epoch Step: 0 Loss: 17.462725\n",
      "Epoch Step: 100 Loss: 20.638805\n",
      "Epoch Step: 200 Loss: 18.728052\n",
      "Epoch Step: 300 Loss: 19.223175\n",
      "Epoch Step: 400 Loss: 17.157764\n",
      "Epoch Step: 500 Loss: 19.276445\n",
      "Epoch Step: 600 Loss: 19.510698\n",
      "Epoch Step: 700 Loss: 19.428209\n",
      "Validation perplexity: 5.180992\n",
      "Epoch 35\n",
      "Epoch Step: 0 Loss: 17.059729\n",
      "Epoch Step: 100 Loss: 16.875065\n",
      "Epoch Step: 200 Loss: 16.679129\n",
      "Epoch Step: 300 Loss: 18.531096\n",
      "Epoch Step: 400 Loss: 18.534670\n",
      "Epoch Step: 500 Loss: 18.894169\n",
      "Epoch Step: 600 Loss: 19.266542\n",
      "Epoch Step: 700 Loss: 17.264725\n",
      "Validation perplexity: 5.039230\n",
      "Epoch 36\n",
      "Epoch Step: 0 Loss: 18.652004\n",
      "Epoch Step: 100 Loss: 17.011444\n",
      "Epoch Step: 200 Loss: 17.831230\n",
      "Epoch Step: 300 Loss: 18.912659\n",
      "Epoch Step: 400 Loss: 17.198359\n",
      "Epoch Step: 500 Loss: 17.211769\n",
      "Epoch Step: 600 Loss: 16.384586\n",
      "Epoch Step: 700 Loss: 17.944519\n",
      "Validation perplexity: 4.841409\n",
      "Epoch 37\n",
      "Epoch Step: 0 Loss: 17.686213\n",
      "Epoch Step: 100 Loss: 17.685974\n",
      "Epoch Step: 200 Loss: 19.651817\n",
      "Epoch Step: 300 Loss: 17.198940\n",
      "Epoch Step: 400 Loss: 17.841751\n",
      "Epoch Step: 500 Loss: 18.269846\n",
      "Epoch Step: 600 Loss: 16.687645\n",
      "Epoch Step: 700 Loss: 18.102314\n",
      "Validation perplexity: 4.780340\n",
      "Epoch 38\n",
      "Epoch Step: 0 Loss: 15.957364\n",
      "Epoch Step: 100 Loss: 17.092669\n",
      "Epoch Step: 200 Loss: 16.452763\n",
      "Epoch Step: 300 Loss: 17.115856\n",
      "Epoch Step: 400 Loss: 18.053423\n",
      "Epoch Step: 500 Loss: 18.488718\n",
      "Epoch Step: 600 Loss: 16.932104\n",
      "Epoch Step: 700 Loss: 18.489906\n",
      "Validation perplexity: 4.574091\n",
      "Epoch 39\n",
      "Epoch Step: 0 Loss: 17.583958\n",
      "Epoch Step: 100 Loss: 18.381695\n",
      "Epoch Step: 200 Loss: 18.135895\n",
      "Epoch Step: 300 Loss: 16.309990\n",
      "Epoch Step: 400 Loss: 15.378511\n",
      "Epoch Step: 500 Loss: 17.949007\n",
      "Epoch Step: 600 Loss: 17.621073\n",
      "Epoch Step: 700 Loss: 16.892057\n",
      "Validation perplexity: 4.383391\n",
      "Epoch 40\n",
      "Epoch Step: 0 Loss: 16.360609\n",
      "Epoch Step: 100 Loss: 16.282682\n",
      "Epoch Step: 200 Loss: 14.885035\n",
      "Epoch Step: 300 Loss: 14.852908\n",
      "Epoch Step: 400 Loss: 17.502691\n",
      "Epoch Step: 500 Loss: 16.545448\n",
      "Epoch Step: 600 Loss: 16.339537\n",
      "Epoch Step: 700 Loss: 14.734973\n",
      "Validation perplexity: 4.205725\n",
      "Epoch 41\n",
      "Epoch Step: 0 Loss: 14.977761\n",
      "Epoch Step: 100 Loss: 14.125322\n",
      "Epoch Step: 200 Loss: 16.067017\n",
      "Epoch Step: 300 Loss: 15.091952\n",
      "Epoch Step: 400 Loss: 14.337639\n",
      "Epoch Step: 500 Loss: 18.417162\n",
      "Epoch Step: 600 Loss: 15.614648\n",
      "Epoch Step: 700 Loss: 15.546961\n",
      "Validation perplexity: 4.075591\n",
      "Epoch 42\n",
      "Epoch Step: 0 Loss: 17.770794\n",
      "Epoch Step: 100 Loss: 14.135544\n",
      "Epoch Step: 200 Loss: 17.174543\n",
      "Epoch Step: 300 Loss: 16.674480\n",
      "Epoch Step: 400 Loss: 15.249743\n",
      "Epoch Step: 500 Loss: 16.766396\n",
      "Epoch Step: 600 Loss: 15.978181\n",
      "Epoch Step: 700 Loss: 14.112450\n",
      "Validation perplexity: 4.018662\n",
      "Epoch 43\n",
      "Epoch Step: 0 Loss: 13.625620\n",
      "Epoch Step: 100 Loss: 16.379557\n",
      "Epoch Step: 200 Loss: 15.863453\n",
      "Epoch Step: 300 Loss: 17.986465\n",
      "Epoch Step: 400 Loss: 16.419348\n",
      "Epoch Step: 500 Loss: 17.049896\n",
      "Epoch Step: 600 Loss: 17.661371\n",
      "Epoch Step: 700 Loss: 14.955771\n",
      "Validation perplexity: 3.923633\n",
      "Epoch 44\n",
      "Epoch Step: 0 Loss: 16.874142\n",
      "Epoch Step: 100 Loss: 15.376920\n",
      "Epoch Step: 200 Loss: 16.415672\n",
      "Epoch Step: 300 Loss: 14.852948\n",
      "Epoch Step: 400 Loss: 17.041986\n",
      "Epoch Step: 500 Loss: 16.058786\n",
      "Epoch Step: 600 Loss: 15.953662\n",
      "Epoch Step: 700 Loss: 14.849629\n",
      "Validation perplexity: 3.763665\n",
      "Epoch 45\n",
      "Epoch Step: 0 Loss: 16.557688\n",
      "Epoch Step: 100 Loss: 14.112531\n",
      "Epoch Step: 200 Loss: 17.811256\n",
      "Epoch Step: 300 Loss: 18.069460\n",
      "Epoch Step: 400 Loss: 15.913533\n",
      "Epoch Step: 500 Loss: 13.520503\n",
      "Epoch Step: 600 Loss: 15.626989\n",
      "Epoch Step: 700 Loss: 16.429504\n",
      "Validation perplexity: 3.634768\n",
      "Epoch 46\n",
      "Epoch Step: 0 Loss: 15.940074\n",
      "Epoch Step: 100 Loss: 17.500460\n",
      "Epoch Step: 200 Loss: 18.283045\n",
      "Epoch Step: 300 Loss: 15.814123\n",
      "Epoch Step: 400 Loss: 14.782760\n",
      "Epoch Step: 500 Loss: 15.897740\n",
      "Epoch Step: 600 Loss: 13.842753\n",
      "Epoch Step: 700 Loss: 14.944950\n",
      "Validation perplexity: 3.607649\n",
      "Epoch 47\n",
      "Epoch Step: 0 Loss: 16.628990\n",
      "Epoch Step: 100 Loss: 14.669544\n",
      "Epoch Step: 200 Loss: 15.112865\n",
      "Epoch Step: 300 Loss: 16.529102\n",
      "Epoch Step: 400 Loss: 13.893866\n",
      "Epoch Step: 500 Loss: 15.711193\n",
      "Epoch Step: 600 Loss: 15.489268\n",
      "Epoch Step: 700 Loss: 13.750154\n",
      "Validation perplexity: 3.490418\n",
      "Epoch 48\n",
      "Epoch Step: 0 Loss: 13.878692\n",
      "Epoch Step: 100 Loss: 16.892111\n",
      "Epoch Step: 200 Loss: 14.507592\n",
      "Epoch Step: 300 Loss: 13.508012\n",
      "Epoch Step: 400 Loss: 16.968025\n",
      "Epoch Step: 500 Loss: 14.797757\n",
      "Epoch Step: 600 Loss: 14.103212\n",
      "Epoch Step: 700 Loss: 14.151592\n",
      "Validation perplexity: 3.393468\n",
      "Epoch 49\n",
      "Epoch Step: 0 Loss: 15.001893\n",
      "Epoch Step: 100 Loss: 16.463165\n",
      "Epoch Step: 200 Loss: 15.068524\n",
      "Epoch Step: 300 Loss: 14.506781\n",
      "Epoch Step: 400 Loss: 14.158655\n",
      "Epoch Step: 500 Loss: 17.584045\n",
      "Epoch Step: 600 Loss: 14.301316\n",
      "Epoch Step: 700 Loss: 16.714964\n",
      "Validation perplexity: 3.329411\n",
      "Epoch 50\n",
      "Epoch Step: 0 Loss: 13.442746\n",
      "Epoch Step: 100 Loss: 15.302402\n",
      "Epoch Step: 200 Loss: 13.874804\n",
      "Epoch Step: 300 Loss: 14.121770\n",
      "Epoch Step: 400 Loss: 16.882881\n",
      "Epoch Step: 500 Loss: 15.714623\n",
      "Epoch Step: 600 Loss: 13.131568\n",
      "Epoch Step: 700 Loss: 13.368196\n",
      "Validation perplexity: 3.225573\n",
      "Epoch 51\n",
      "Epoch Step: 0 Loss: 13.616126\n",
      "Epoch Step: 100 Loss: 12.091525\n",
      "Epoch Step: 200 Loss: 13.549526\n",
      "Epoch Step: 300 Loss: 14.106633\n",
      "Epoch Step: 400 Loss: 11.030601\n",
      "Epoch Step: 500 Loss: 16.267632\n",
      "Epoch Step: 600 Loss: 15.179927\n",
      "Epoch Step: 700 Loss: 14.245945\n",
      "Validation perplexity: 3.159818\n",
      "Epoch 52\n",
      "Epoch Step: 0 Loss: 15.208710\n",
      "Epoch Step: 100 Loss: 13.129807\n",
      "Epoch Step: 200 Loss: 14.736259\n",
      "Epoch Step: 300 Loss: 16.106741\n",
      "Epoch Step: 400 Loss: 15.082935\n",
      "Epoch Step: 500 Loss: 13.990831\n",
      "Epoch Step: 600 Loss: 15.277420\n",
      "Epoch Step: 700 Loss: 12.659895\n",
      "Validation perplexity: 3.085945\n",
      "Epoch 53\n",
      "Epoch Step: 0 Loss: 14.352407\n",
      "Epoch Step: 100 Loss: 13.619600\n",
      "Epoch Step: 200 Loss: 13.732153\n",
      "Epoch Step: 300 Loss: 13.529262\n",
      "Epoch Step: 400 Loss: 12.464168\n",
      "Epoch Step: 500 Loss: 13.069148\n",
      "Epoch Step: 600 Loss: 14.057594\n",
      "Epoch Step: 700 Loss: 12.770706\n",
      "Validation perplexity: 3.004533\n",
      "Epoch 54\n",
      "Epoch Step: 0 Loss: 14.165680\n",
      "Epoch Step: 100 Loss: 14.363449\n",
      "Epoch Step: 200 Loss: 15.517643\n",
      "Epoch Step: 300 Loss: 14.192081\n",
      "Epoch Step: 400 Loss: 13.691204\n",
      "Epoch Step: 500 Loss: 11.220455\n",
      "Epoch Step: 600 Loss: 16.493422\n",
      "Epoch Step: 700 Loss: 14.239801\n",
      "Validation perplexity: 2.914039\n",
      "Epoch 55\n",
      "Epoch Step: 0 Loss: 13.491163\n",
      "Epoch Step: 100 Loss: 13.072419\n",
      "Epoch Step: 200 Loss: 12.871275\n",
      "Epoch Step: 300 Loss: 15.973206\n",
      "Epoch Step: 400 Loss: 13.578107\n",
      "Epoch Step: 500 Loss: 12.941659\n",
      "Epoch Step: 600 Loss: 15.666170\n",
      "Epoch Step: 700 Loss: 13.021294\n",
      "Validation perplexity: 2.872383\n",
      "Epoch 56\n",
      "Epoch Step: 0 Loss: 14.438663\n",
      "Epoch Step: 100 Loss: 11.222799\n",
      "Epoch Step: 200 Loss: 13.659509\n",
      "Epoch Step: 300 Loss: 11.504926\n",
      "Epoch Step: 400 Loss: 12.768744\n",
      "Epoch Step: 500 Loss: 12.563115\n",
      "Epoch Step: 600 Loss: 12.990462\n",
      "Epoch Step: 700 Loss: 14.177577\n",
      "Validation perplexity: 2.835665\n",
      "Epoch 57\n",
      "Epoch Step: 0 Loss: 13.465489\n",
      "Epoch Step: 100 Loss: 14.740372\n",
      "Epoch Step: 200 Loss: 14.771005\n",
      "Epoch Step: 300 Loss: 13.444674\n",
      "Epoch Step: 400 Loss: 13.413854\n",
      "Epoch Step: 500 Loss: 14.309317\n",
      "Epoch Step: 600 Loss: 12.146519\n",
      "Epoch Step: 700 Loss: 13.696680\n",
      "Validation perplexity: 2.820267\n",
      "Epoch 58\n",
      "Epoch Step: 0 Loss: 12.554234\n",
      "Epoch Step: 100 Loss: 12.767912\n",
      "Epoch Step: 200 Loss: 14.023080\n",
      "Epoch Step: 300 Loss: 14.877626\n",
      "Epoch Step: 400 Loss: 12.898275\n",
      "Epoch Step: 500 Loss: 15.415608\n",
      "Epoch Step: 600 Loss: 12.489408\n",
      "Epoch Step: 700 Loss: 13.178203\n",
      "Validation perplexity: 2.729058\n",
      "Epoch 59\n",
      "Epoch Step: 0 Loss: 12.273672\n",
      "Epoch Step: 100 Loss: 14.656730\n",
      "Epoch Step: 200 Loss: 13.437900\n",
      "Epoch Step: 300 Loss: 12.107482\n",
      "Epoch Step: 400 Loss: 11.080262\n",
      "Epoch Step: 500 Loss: 11.937473\n",
      "Epoch Step: 600 Loss: 13.508640\n",
      "Epoch Step: 700 Loss: 13.001297\n",
      "Validation perplexity: 2.696942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "attention = BahAttention(hidden_size)\n",
    "attn_seq2seq = EncoderAttentionDecoder(\n",
    "  encoder=EncoderLSTM(len(src_vocab), hidden_size, num_layers, bidirection, dropout),\n",
    "  decoder=AttentionDecoder(len(trg_vocab), hidden_size, num_layers, feed_ratio, dropout, attention), \n",
    "  #decoder=AttentionDecoder(len(trg_vocab), hidden_size, num_layers, feed_ratio, dropout), \n",
    "  feed_ratio = feed_ratio,\n",
    "  generator=Generator(hidden_size, len(trg_vocab))).to(device)\n",
    "\n",
    "pure_seq2seq = EncoderDecoder(\n",
    "  encoder=EncoderRNN(len(src_vocab), hidden_size),\n",
    "  decoder=DecoderRNN(len(trg_vocab), hidden_size), \n",
    "  feed_ratio = feed_ratio,\n",
    "  generator=Generator(hidden_size, len(trg_vocab))).to(device)\n",
    "\n",
    "\n",
    "attn_dev_ppls = train(attn_seq2seq, num_epochs, learning_rate, 100, clipping_value, early_stop)\n",
    "\n",
    "#pure_dev_ppls = train(pure_seq2seq, num_epochs, learning_rate, 100, clipping_value, early_stop)\n",
    "\n",
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "\n",
    "\n",
    "#torch.save(attn_seq2seq, 'seq2seq.model')\n",
    "torch.save(pure_seq2seq, 'seq2seq_pure.model')\n",
    "plot_perplexity(attn_dev_ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot_perplexity(pure_dev_ppls)\n",
    "plot_perplexity(attn_dev_ppls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下设置变量用于绘制图标和跟踪进度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要进行实际训练，我们会多次调用训练函数，并在进行过程中打印中间信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Perplexity per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.plot(pure_dev_ppls, label='pure model')\n",
    "#plt.plot(attn_dev_ppls, label='attn model')\n",
    "plt.plot(attn_dev_ppls, label='simple model')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "def greedy_decode(model, src_ids, src_lengths, max_len, pad_index = 0):\n",
    "    \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, encoder_final = model.encoder(src_ids, src_lengths)\n",
    "        decoder_input = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
    "\n",
    "    output = []\n",
    "    hidden = None\n",
    "    decoder_hidden = encoder_final\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            if model == pure_seq2seq:\n",
    "                decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "            if model == attn_seq2seq:\n",
    "                #print(decoder_input)\n",
    "                src_mask = (src_ids != pad_index).unsqueeze(1)\n",
    "                decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden, encoder_outputs, encoder_final, src_mask)\n",
    "            \n",
    "            prob = model.generator(decoder_output[:, -1])\n",
    "            \n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        #print(prob_val)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        decoder_input = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n",
    "\n",
    "    output = np.array(output)\n",
    "    #print(output)\n",
    "\n",
    "    # Cut off everything starting from </s>.\n",
    "    first_eos = np.where(output == EOS_INDEX)[0]\n",
    "    if len(first_eos) > 0:\n",
    "        output = output[:first_eos[0]]\n",
    "\n",
    "    #print(np.array(attn_scores))\n",
    "    return output\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab):\n",
    "    return [vocab[i] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(model, src_ids, src_lengths, max_len, beam_width, pad_index = 0):\n",
    "    \"\"\"Greedily decode a sentence for EncoderDecoder.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, encoder_final = model.encoder(src_ids, src_lengths)\n",
    "        decoder_input = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
    "\n",
    "    output = []\n",
    "    hidden = None\n",
    "    decoder_hidden = encoder_final\n",
    "    final_candidates = []\n",
    "    candidates = [(0, decoder_input, decoder_hidden, [])]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        if beam_width == 0:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            next_candidates = []\n",
    "            for log_p, decoder_input, decoder_hidden, words in candidates:\n",
    "                #if model == attn_seq2seq:\n",
    "                src_mask = (src_ids != pad_index).unsqueeze(1)\n",
    "                decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden, encoder_outputs, encoder_final, src_mask)\n",
    "                #print(decoder_output[:, -1].size())\n",
    "                prob = model.generator(decoder_output[:, -1])\n",
    "                topv, topi = prob.data.topk(beam_width)\n",
    "                #print(topi)\n",
    "                #print(torch.max(prob, dim=1))\n",
    "                topv = torch.add(topv, log_p)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    if words != [] and words[-1] == topi[0][i].data.item():\n",
    "#                         topv[0][i] = topv[0][i]*2\n",
    "                        continue\n",
    "                    next_candidates.append([topv[0][i], topi[0][i], decoder_hidden, words + [topi[0][i].data.item()]])\n",
    "            \n",
    "            #print(next_candidates.size())\n",
    "            next_candidates.sort(reverse = True)\n",
    "            #print('sorted candidates')\n",
    "            #print(next_candidates)\n",
    "            #next_candidates = next_candidates[:beam_width]\n",
    "            candidates = []\n",
    "            for i in range(len(next_candidates)):\n",
    "                if len(candidates) == beam_width:\n",
    "                    break\n",
    "                #first_eos = np.where(np.array(next_candidates[i][2]) == EOS_INDEX)[0]\n",
    "                if EOS_INDEX == next_candidates[i][1]:\n",
    "                    final_candidates.append([next_candidates[i][0], next_candidates[i][3]])\n",
    "                    beam_width -= 1\n",
    "                else:\n",
    "                    next_candidates[i][1] = torch.ones(1, 1).type_as(src_ids).fill_(next_candidates[i][1].data.item())\n",
    "                    candidates.append(next_candidates[i])\n",
    "    \n",
    "    max_log_p = -float('inf')\n",
    "    best_decode = []\n",
    "    for candidate in final_candidates:\n",
    "        #print(candidate[2])\n",
    "        candidate[0] = candidate[0]/len(candidate[1])\n",
    "        #print(candidate[0])\n",
    "        if candidate[0] > max_log_p:\n",
    "            max_log_p = candidate[0]\n",
    "            best_decode = candidate[1]\n",
    "    \n",
    "    best_decode = np.array(best_decode)\n",
    "    #print(best_decode)\n",
    "    first_eos = np.where(best_decode == EOS_INDEX)[0]\n",
    "    #print(first_eos[0])\n",
    "    if len(first_eos) > 0:\n",
    "        best_decode = best_decode[:first_eos[0]]\n",
    "        \n",
    "    return best_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 5\n",
    "\n",
    "def print_examples(model, data_loader, n=5,\n",
    "                   max_len=MAX_LENGTH, \n",
    "                   src_vocab_set=src_vocab, trg_vocab_set=trg_vocab):\n",
    "    \"\"\"Prints `n` examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for i, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n",
    "        pred = beam_decode(model, src_ids.to(device), src_lengths.to(device),\n",
    "                               max_len, beam_width)\n",
    "        \n",
    "#         pred = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n",
    "#                                max_len)\n",
    "\n",
    "        #preds.append(result)\n",
    "        #weights.append(attn_scores)\n",
    "\n",
    "        # remove <s>\n",
    "        src_ids = src_ids[0, 1:]\n",
    "        trg_ids = trg_ids[0, 1:]\n",
    "\n",
    "        # remove </s> and <pad>\n",
    "        src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
    "        trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
    "\n",
    "        src_w = lookup_words(src_ids, src_vocab_set)\n",
    "        trg_w = lookup_words(trg_ids, trg_vocab_set)\n",
    "        pred_w = lookup_words(pred, trg_vocab_set)    \n",
    "        \n",
    "        if i > 30 and i < 50:\n",
    "            print(\"Example #%d\" % (i + 1))\n",
    "            print(\"Src : \", \" \".join(lookup_words(src_ids, vocab=src_vocab_set)))\n",
    "            print(\"Trg : \", \" \".join(lookup_words(trg_ids, vocab=trg_vocab_set)))\n",
    "            print(\"Pred: \", \" \".join(lookup_words(pred, vocab=trg_vocab_set)))\n",
    "        #print()\n",
    "\n",
    "        if i >= 50:\n",
    "            break\n",
    "    \n",
    "    return src_w, pred_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1495/5063 [03:22<07:09,  8.30it/s]"
     ]
    }
   ],
   "source": [
    "#import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_BLEU(model, data_loader):\n",
    "    bleu_score = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    mapping = {}\n",
    "    for i, (src_ids, src_lengths, trg_ids, _) in enumerate(data_loader):\n",
    "        src_ids = src_ids[0, 1:]\n",
    "        trg_ids = trg_ids[0, 1:]\n",
    "        \n",
    "        src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
    "        trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
    "        \n",
    "        src_w = lookup_words(src_ids, src_vocab)\n",
    "        trg_w = lookup_words(trg_ids, trg_vocab)\n",
    "        \n",
    "        src = \" \".join(src_w)\n",
    "        if src not in mapping:\n",
    "            mapping[src] = [trg_w]\n",
    "        else:\n",
    "            mapping[src].append(trg_w)\n",
    "            \n",
    "    for src_ids, src_lengths, trg_ids, _ in tqdm(data_loader):\n",
    "        result = beam_decode(model, src_ids.to(device), src_lengths.to(device),\n",
    "                               MAX_LENGTH, beam_width)\n",
    "#         result = greedy_decode(model, src_ids.to(device), src_lengths.to(device),\n",
    "#                                MAX_LENGTH)\n",
    "        # remove <s>\n",
    "        src_ids = src_ids[0, 1:]\n",
    "        trg_ids = trg_ids[0, 1:]\n",
    "        # remove </s> and <pad>\n",
    "        src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
    "        trg_ids = trg_ids[:np.where(trg_ids == EOS_INDEX)[0][0]]\n",
    "        \n",
    "        src_list = lookup_words(src_ids, vocab = src_vocab)\n",
    "        pred_list = lookup_words(result, vocab=trg_vocab)\n",
    "        targ_list = lookup_words(trg_ids, vocab=trg_vocab)\n",
    "        \n",
    "        src = \" \".join(lookup_words(src_ids, vocab=src_vocab))\n",
    "        pred = \" \".join(lookup_words(result, vocab=trg_vocab))\n",
    "        targ = \" \".join(lookup_words(trg_ids, vocab=trg_vocab))\n",
    "        \n",
    "        if src in mapping:\n",
    "            bleu_score.append(sentence_bleu(mapping[src], pred_list))\n",
    "        #bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]], .01).score)\n",
    "        #bleu_score.append(sentence_bleu([targ_list], pred_list))\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "# test_set = MTDataset(test_src_sentences_list, src_vocab,\n",
    "#                      test_trg_sentences_list, trg_vocab, sampling=1.)\n",
    "# test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8,\n",
    "#                                    shuffle=False)\n",
    "\n",
    "example_set = MTDataset(val_src_sentences_list, src_vocab,\n",
    "                        val_trg_sentences_list, trg_vocab)\n",
    "\n",
    "example_data_loader = data.DataLoader(val_set, batch_size=1, num_workers=1,\n",
    "                                      shuffle=False)\n",
    "\n",
    "print('BLEU score: %f' % (np.mean(compute_BLEU(attn_seq2seq,\n",
    "                                               example_data_loader))))\n",
    "\n",
    "# print('BLEU score: %f' % (np.mean(compute_BLEU(pure_seq2seq,\n",
    "#                                                example_data_loader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lookup_words([14], trg_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #32\n",
      "Src :  她 向 我 求 助 。\n",
      "Trg :  she asked me for help .\n",
      "Pred:  she asked me for help .\n",
      "Example #33\n",
      "Src :  我 想 我 今 晚 会 洗 澡 。\n",
      "Trg :  i think i ll take a bath tonight .\n",
      "Pred:  i think i ll take a bath tonight .\n",
      "Example #34\n",
      "Src :  我 努 力 地 平 息 他 们 不 断 加 温 的 争 执 。\n",
      "Trg :  i tried very hard to put an end to their heated argument .\n",
      "Pred:  i tried surprised to that to take rid of them of them .\n",
      "Example #35\n",
      "Src :  年 轻 人 伸 出 他 的 手 ， 接 着 我 和 他 握 了 手 。\n",
      "Trg :  the young man put out his hand and i shook it .\n",
      "Pred:  the man were his hand his hand to him out .\n",
      "Example #36\n",
      "Src :  月 光 真 美 。\n",
      "Trg :  the moonlight is beautiful .\n",
      "Pred:  the moonlight is really beautiful .\n",
      "Example #37\n",
      "Src :  我 差 点 大 声 笑 了 出 来 。\n",
      "Trg :  i almost laughed out loud .\n",
      "Pred:  i almost laughed out .\n",
      "Example #38\n",
      "Src :  汤 姆 是 不 是 麻 烦 事 很 多 ？\n",
      "Trg :  is tom in a lot of trouble ?\n",
      "Pred:  is tom any a lot of that ?\n",
      "Example #39\n",
      "Src :  我 收 集 很 多 邮 票 。\n",
      "Trg :  i have a large collection of stamps .\n",
      "Pred:  i have a lot of stamps .\n",
      "Example #40\n",
      "Src :  您 叫 什 么 名 字 ？\n",
      "Trg :  what is your name ?\n",
      "Pred:  what s your name ?\n",
      "Example #41\n",
      "Src :  他 父 亲 是 日 本 人 。\n",
      "Trg :  his father is japanese .\n",
      "Pred:  his father is japanese .\n",
      "Example #42\n",
      "Src :  但 愿 我 见 过 她 。\n",
      "Trg :  i wish i had seen her .\n",
      "Pred:  i wish i had seen her .\n",
      "Example #43\n",
      "Src :  他 们 在 图 书 馆 装 备 了 新 的 书 籍 。\n",
      "Trg :  they furnished the library with new books .\n",
      "Pred:  they furnished the book in new books .\n",
      "Example #44\n",
      "Src :  我 肯 定 这 是 淡 水 鱼 。\n",
      "Trg :  i m sure that this is a fresh water fish .\n",
      "Pred:  i m sure that this is a cold fish .\n",
      "Example #45\n",
      "Src :  我 注 册 了 ， 现 在 该 干 什 么 呢 ？\n",
      "Trg :  what do i have to do now that i m registered ?\n",
      "Pred:  what do i have what do now ? i m done ? now ?\n",
      "Example #46\n",
      "Src :  如 果 你 开 心 ， 我 也 就 开 心 。\n",
      "Trg :  i m happy if you re happy .\n",
      "Pred:  i m happy if i re happy .\n",
      "Example #47\n",
      "Src :  我 能 用 一 下 你 们 的 厕 所 吗 ？\n",
      "Trg :  may i use your toilet ?\n",
      "Pred:  may i use your toilet ?\n",
      "Example #48\n",
      "Src :  我 一 直 在 学 开 车 。\n",
      "Trg :  i have been learning to drive .\n",
      "Pred:  i ve always learning to drive .\n",
      "Example #49\n",
      "Src :  损 失 是 无 法 估 计 的 。\n",
      "Trg :  the losses are incalculable .\n",
      "Pred:  the losses is incalculable .\n",
      "Example #50\n",
      "Src :  请 你 把 胡 椒 递 给 我 好 吗 ?\n",
      "Trg :  could you please pass me the pepper ?\n",
      "Pred:  could you please pass me the salt ?\n"
     ]
    }
   ],
   "source": [
    "src_w, pred_w = print_examples(attn_seq2seq, example_data_loader, 15, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1\n",
      "Pred:  why so i not a good time i can study t study english ?\n",
      "Example #2\n",
      "Pred:  she helped me love her in her .\n",
      "Example #3\n",
      "Pred:  this bottle of yogurt isn t lactose .\n",
      "Example #4\n",
      "Pred:  this bottle of yogurt isn t lactose .\n",
      "Example #5\n",
      "Pred:  i can t help you .\n",
      "Example #6\n",
      "Pred:  i can t help you .\n",
      "Example #7\n",
      "Pred:  tom is not a good driver .\n",
      "Example #8\n",
      "Pred:  i can speak spanish putonghua and southwestern wu chinese wu mandarin .\n",
      "Example #9\n",
      "Pred:  this question is not that simple .\n",
      "Example #10\n",
      "Pred:  he can speak french but he can speak french .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, src_sentences, src_vocab, sampling=1.):\n",
    "        self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
    "        \n",
    "        self.src_vocab = src_vocab\n",
    "        \n",
    "        self.max_src_seq_length = MAX_LENGTH+2   # max length plus eos and sos\n",
    "\n",
    "        self.src_v2id = {v : i for i, v in enumerate(src_vocab)}\n",
    "        self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_sent = self.src_sentences[index]\n",
    "        src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n",
    "        src_id = []\n",
    "        for w in src_sent:\n",
    "            if w not in self.src_vocab:\n",
    "                w = '<unk>'\n",
    "            src_id.append(self.src_v2id[w])\n",
    "        src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
    "                  (self.max_src_seq_length - src_len))\n",
    "\n",
    "        return torch.tensor(src_id), src_len\n",
    "\n",
    "\n",
    "test_src_sentences_list = []\n",
    "test_src_vocab = []\n",
    "with open('test.txt', \"r\", encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        src = [normalize_string(s) for s in line]\n",
    "        #src = zhconv.convert(src, 'zh-hans')\n",
    "        #print(trg)\n",
    "        temp = []\n",
    "        for w in src:\n",
    "            if w != ' ':\n",
    "                temp.append(w)\n",
    "            if w not in src_vocab:\n",
    "                test_src_vocab.append(w)\n",
    "        #if temp not in test_src_sentences_list:\n",
    "        test_src_sentences_list.append(temp)\n",
    "\n",
    "test_set = TestDataset(test_src_sentences_list, src_vocab, sampling=1.)\n",
    "test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8, shuffle=False)\n",
    "\n",
    "def translate_test_examples(model, data_loader, n_print = 10, max_len=MAX_LENGTH,\n",
    "                   src_vocab_set=test_src_vocab, trg_vocab_set=trg_vocab):\n",
    "    \"\"\"Prints `n` examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for i, (src_ids, src_lengths) in enumerate(data_loader):\n",
    "        pred = beam_decode(model, src_ids.to(device), src_lengths.to(device), max_len, beam_width)\n",
    "\n",
    "        #preds.append(result)\n",
    "        #weights.append(attn_scores)\n",
    "\n",
    "        # remove <s>\n",
    "        src_ids = src_ids[0, 1:]\n",
    "\n",
    "        # remove </s> and <pad>\n",
    "        src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n",
    "\n",
    "        src_w = \" \".join(lookup_words(src_ids, vocab=src_vocab_set))\n",
    "        #trg_w = lookup_words(trg_ids, trg_vocab_set)\n",
    "        pred_w = \" \".join(lookup_words(pred, vocab=trg_vocab_set))\n",
    "\n",
    "        #print(\"Example #%d\" % (i + 1))\n",
    "        #print(\"Src : \", \" \".join(lookup_words(src_ids, vocab=src_vocab_set)))\n",
    "        #print(\"Pred: \", \" \".join(lookup_words(pred, vocab=trg_vocab_set)))\n",
    "        preds.append(pred_w)\n",
    "        \n",
    "        if i < n_print:\n",
    "            print(\"Example #%d\" % (i + 1))\n",
    "            print(\"Pred: \", pred_w)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "preds = translate_test_examples(attn_seq2seq, test_data_loader, 10, MAX_LENGTH)\n",
    "with open('test_results_beam1.txt', 'w', encoding = 'utf8') as outfile:\n",
    "    json.dump(preds, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate(sentence, encoder, decoder, beam_width, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    #target_variable = variable_from_sentence(input_lang, target)\n",
    "    \n",
    "    # Run through encoder\n",
    "    #encoder_hidden = encoder.init_hidden()\n",
    "    #encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable)\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token]]) # SOS\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    candidates = [(0, decoder_input, [])]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    #decoded_words_idx = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    #log_p = 0\n",
    "    # Run through decoder\n",
    "    final_candidates = []\n",
    "    for di in range(max_length):\n",
    "        #if len(final_candidates) == beam_width or beam_width <= 0:\n",
    "            #break\n",
    "            \n",
    "        next_candidates = []\n",
    "        for log_p, decoder_input, words in candidates:\n",
    "            #decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            for idx in range(output_lang.n_words):\n",
    "                w = output_lang.index2word[idx]\n",
    "                if w == 'SOS' or 'UNK':\n",
    "                    continue\n",
    "                elif w == 'EOS':\n",
    "                    count = len(pairs)\n",
    "                    decoder_output[0, idx] = decoder_output[0, idx]*count/1000\n",
    "                else:\n",
    "                    count = output_lang.word2count[w]\n",
    "                    if count > 5000:\n",
    "                        decoder_output[0, idx] = decoder_output[0, idx]*count/1000\n",
    "\n",
    "            # Choose top word from output\n",
    "            topv, topi = decoder_output.data.topk(beam_width)\n",
    "            topv = torch.add(topv, log_p)\n",
    "            for i in range(beam_width):\n",
    "                #print(topi[0][i])\n",
    "                word_i = output_lang.index2word[topi[0][i].item()]\n",
    "                #print(word_i)\n",
    "                next_candidates.append([topv[0][i], topi[0][i], words + [word_i]])\n",
    "        \n",
    "        next_candidates.sort(reverse = True)\n",
    "        next_candidates = next_candidates[:beam_width]\n",
    "        candidates = []\n",
    "        for i in range(len(next_candidates)):\n",
    "            if EOS_token == next_candidates[i][1] or di == max_length-1:\n",
    "                beam_width -= 1\n",
    "                final_candidates.append(next_candidates[i])\n",
    "            else:\n",
    "                next_candidates[i][1] = torch.LongTensor([[next_candidates[i][1]]])\n",
    "                if USE_CUDA: next_candidates[i][1] = next_candidates[i][1].cuda()\n",
    "                candidates.append(next_candidates[i])\n",
    "        \n",
    "#         # Next input is chosen word\n",
    "#         decoder_input = torch.LongTensor([[ni]])\n",
    "#         if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    max_log_p = -float('inf')\n",
    "    best_decode = []\n",
    "    for candidate in final_candidates:\n",
    "        print(candidate[2])\n",
    "        candidate[0] = candidate[0].item()/len(candidate[2])\n",
    "        if candidate[0] > max_log_p:\n",
    "            max_log_p = candidate[0]\n",
    "            best_decode = candidate[2]\n",
    "        \n",
    "    return best_decode\n",
    "\n",
    "#words = evaluate(pairs[1][0], encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制训练loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选取一个句子进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 5\n",
    "def evaluate_randomly(encoder, decoder):\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    #output_words = beam_evaluate(pair[0], encoder, decoder, beam_width)\n",
    "    output_words = beam_evaluate(pair[0], encoder, decoder, beam_width)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    evaluate_randomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机的验证只是一个简单的例子，为了能系统性的完成测试数据的翻译，这里仍需要实现一个新的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
